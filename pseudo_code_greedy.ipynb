{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo Code for **GREEDY** algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from threading import Thread, Barrier, Lock, get_ident\n",
    "from queue import Queue\n",
    "import copy\n",
    "raw = pd.read_pickle(\"lb_algo_dataframe.bz2\")\n",
    "nprocs = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "With a single AllGather MPI operation all load_imbalances for all processors are synced across all processors. \n",
    "Processors update this ```load_imbalance``` list consistently during the partitioning\n",
    "\"\"\"\n",
    "avg_weight = raw[' weight'].sum()/nprocs\n",
    "load_imbalance = [ (raw[raw[' source_proc'] == x][' weight'].sum()-avg_weight)/avg_weight for x in range(nprocs) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The first part of the algorithm determines how many block are computed locally, \n",
    "for the remaining blocks we execute the iterative **GREEDY** algoritm\n",
    "\n",
    "Calculate for each process which grid cells do not need to be offloaded and can be computed locally.\n",
    "Set the compute_proc property to -1 for each cell that can be offloaded\n",
    "\"\"\"\n",
    "def initial_calc(x):\n",
    "  proc = raw[raw[' source_proc'] == x].copy()\n",
    "  proc = proc.reset_index(drop=True)\n",
    "  proc[' compute_proc'] = x\n",
    "  weight = 0\n",
    "  for y in range(proc.shape[0]):\n",
    "    weight += proc.iloc[y][' weight']\n",
    "    if weight > avg_weight:\n",
    "      proc.loc[y+1:,' compute_proc'] = -1\n",
    "      print(\"x\",end=\"\", flush=True)\n",
    "      break\n",
    "  return proc, proc.shape[0]-(y+1), weight\n",
    "  \n",
    "processed = Pool(8).map(initial_calc,range(nprocs))\n",
    "print(\"\\ninitial stage done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the iterative greedy algorithm, MPI is simulated by point-to-point queues between all threads. \n",
    "The result is stored in a shared array, in the actual algorithm this is kept locally\n",
    "\"\"\"\n",
    "\n",
    "#lists (one for each processor)\n",
    "weights, to_offload, cur_weight = map(list,zip(*processed))\n",
    "\n",
    "#point to point 'mpi' communication\n",
    "MPI = [[Queue()for _ in range(nprocs)]for _ in range(nprocs)]\n",
    "\n",
    "#resulting partitioning after loadbalancing\n",
    "new_weights = [None]*nprocs\n",
    "\n",
    "\"\"\"\n",
    "*L (load imbalance):* is consistent between all processors during the algorithm\n",
    "*weights:* weight and compute location of resident cells, \n",
    "*cur_weight:* cumulative weight of cells that will be computed on this processor\n",
    "*to_offload:* number of resident cells that have not yet been assigned to a processor\n",
    "\"\"\"\n",
    "def GREEDY(our_proc,L,weights, cur_weight, to_offload):\n",
    "  # In the worst case we communicate with everyone\n",
    "  for i in range(1,nprocs): \n",
    "    our_val = L[our_proc]\n",
    "    # We try to offload to next_proc, and try to receive from prev_proc\n",
    "    next_proc = (our_proc+i)%nprocs\n",
    "    prev_proc = (our_proc-i+nprocs)%nprocs\n",
    "\n",
    "    # Send information about offloadable blocks to the next process\n",
    "    MPI[our_proc][next_proc].put(to_offload)\n",
    "    MPI[our_proc][next_proc].put(weights.loc[weights.shape[0]-to_offload:,' weight'])\n",
    "   \n",
    "    # Accept cells if we have room\n",
    "    weight_size = MPI[prev_proc][our_proc].get()\n",
    "    their_weights = MPI[prev_proc][our_proc].get()\n",
    "    offloaded = 0\n",
    "    for y in range(weight_size):\n",
    "      if cur_weight > avg_weight:\n",
    "        break\n",
    "      offloaded += 1\n",
    "      cur_weight += their_weights.iloc[y]\n",
    "    MPI[our_proc][prev_proc].put(offloaded)\n",
    "\n",
    "    # Update information about offloaded cells\n",
    "    offloaded = MPI[next_proc][our_proc].get()\n",
    "    weights.loc[weights.shape[0]-to_offload:weights.shape[0]-to_offload+offloaded,' compute_proc'] = next_proc\n",
    "    to_offload -= offloaded\n",
    "\n",
    "    # update L (load imbalance) for every process\n",
    "    for j in range(nprocs):\n",
    "      k = (j+i)%nprocs\n",
    "      if L[j] <= 0:\n",
    "        continue\n",
    "      if (L[j] * L[k]) >= 0:\n",
    "        continue\n",
    "      m = min(abs(L[j]),abs(L[k]))\n",
    "      L[j] -= m\n",
    "      L[k] += m\n",
    "        \n",
    "    # Stop Conditions \n",
    "    if (max(L) < 0.02):\n",
    "      new_weights[our_proc] = weights\n",
    "      return \n",
    "      \n",
    "  new_weights[our_proc] = weights\n",
    "  \n",
    "#instead of MPI, for this example we use threads  \n",
    "threads = [Thread(target=GREEDY,args=( x, copy.deepcopy(load_imbalance), copy.deepcopy(weights[x])\n",
    "                                     , copy.deepcopy(cur_weight[x]), copy.deepcopy(to_offload[x]))) for x in range(nprocs)]\n",
    "for i in range(len(threads)):\n",
    "  threads[i].start()\n",
    "for i in range(len(threads)):\n",
    "  threads[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The result can be inspected via *new_weights*\n",
    "\"\"\"\n",
    "new_weights[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPy3",
   "language": "python",
   "name": "pypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
